{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e74ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# February 2024\n",
    "# Resampling and tokenizing text \n",
    "# Violeta Berdejo-Espinola"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8000d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# linting\n",
    "# !nbqa pylint 1.pre_process_main_text.ipynb\n",
    "\n",
    "# background theme \n",
    "# !jt -t monokai -cellw 90% #grade3\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import mpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f2bf70",
   "metadata": {},
   "source": [
    "# read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b39a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean corpus\n",
    "\n",
    "corpus = mpu.io.read('../data/corpus_clean.pickle')\n",
    "pos = mpu.io.read('../data/pos.pickle')\n",
    "neg = mpu.io.read('../data/neg_complete.pickle')\n",
    "\n",
    "x = pos + neg\n",
    "y = [1] * len(pos) + [0] * len(neg)\n",
    "\n",
    "print(len(y))\n",
    "print(len(neg))\n",
    "\n",
    "# raw corpus\n",
    "\n",
    "corpus_raw = mpu.io.read('../data/corpus_raw.pickle')\n",
    "\n",
    "pos_raw = corpus_raw[0:62]\n",
    "neg_raw = corpus_raw[62:5020]\n",
    "\n",
    "x_raw = corpus_raw\n",
    "y_raw = y\n",
    "\n",
    "print(len(y))\n",
    "print(len(neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26f13f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=42)\n",
    "x_train_r, x_test_r, y_train_r, y_test_r = train_test_split(x_raw, y_raw, test_size=0.20, random_state=42)\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "cntt = Counter()\n",
    "\n",
    "for instance_per_class in y_train:\n",
    "    cntt[instance_per_class] += 1\n",
    "\n",
    "cntt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf48735c",
   "metadata": {},
   "source": [
    "# feature extraction\n",
    "\n",
    "https://stackoverflow.com/questions/62812198/valueerror-in-while-predict-where-test-data-is-having-different-shape-of-word-ve\n",
    "\n",
    "transformer tokenizer: https://huggingface.co/transformers/v2.11.0/main_classes/tokenizer.html#:~:text=max_length%20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bb740b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer # uses one-dim array of strings ~ shape (n,)\n",
    "from sklearn.feature_extraction.text import CountVectorizer # returns arrays\n",
    "\n",
    "vect_tfidf = TfidfVectorizer()\n",
    "vect_cv = CountVectorizer()\n",
    "\n",
    "X_train_cv = vect_cv.fit_transform(x_train)                    #fit: tokenize & buid vocab (turn object into an estimator)\n",
    "X_train_tfidf = vect_tfidf.fit_transform(x_train)              #transform: instances into matrices\n",
    "X_test_cv = vect_cv.transform(x_test)\n",
    "X_test_tfidf = vect_tfidf.transform(x_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64c7958",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sentence_transformers import SentenceTransformer # returns dict of tensors\n",
    "\n",
    "embed_model = SentenceTransformer('distiluse-base-multilingual-cased-v1') \n",
    "\n",
    "embedding_train = embed_model.encode(x_train_r)\n",
    "embedding_test = embed_model.encode(x_test_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f90ae79-c9ea-4318-97d0-e041e33e1576",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base')\n",
    "     \n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch, padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
    "    # returns dictionary with two key:value. input ids:tensors and attention mask:tensors both of them contain tensors\n",
    "    \n",
    "X_train_xlm = tokenize(x_train_r)                              \n",
    "X_test_xlm = tokenize(x_test_r)\n",
    "\n",
    "y_train_xlm = torch.tensor(y_train_r) \n",
    "y_test_xlm = torch.tensor(y_test_r) \n",
    "\n",
    "a = X_train_xlm['input_ids'].size()\n",
    "b = X_test_xlm['input_ids'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce3f9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('document-term matrix\\n')\n",
    "print(f'count based vectors - cv & tfidf:\\n {X_train_cv.shape, X_test_cv.shape} \\n {X_train_tfidf.shape, X_test_tfidf.shape}\\n')\n",
    "print(f'embedding - sentence transformer:\\n {embedding_train.shape, embedding_test.shape}\\n')\n",
    "print(f'embedding - xlm roberta:\\n {a} {b}\\n') # size is batch_size, n_tokens\n",
    "\n",
    "# transformers \n",
    "\n",
    "print(torch.is_tensor(X_train_xlm))\n",
    "print(type(X_train_xlm))\n",
    "\n",
    "print(f'xlm vocabulary size: {tokenizer.vocab_size} \\nmodel context size: {tokenizer.model_max_length}\\nmodel input {tokenizer.model_input_names}\\n')\n",
    "print(f'xlm input ids:\\n {X_train_xlm.input_ids}\\nxlm attention masks:\\n {X_train_xlm.attention_mask}\\n')\n",
    "\n",
    "print('let\\'s explore an example:\\n')\n",
    "print(X_train_xlm['input_ids'][10].size())\n",
    "\n",
    "# the input sequence of each batch is padded [1] to the maximum sequence length in the batch (model context size)\n",
    "# the attention mask array is used to ignore the paddded areas of the betch \n",
    "print(X_train_xlm['input_ids'][0])\n",
    "print(tokenizer.convert_ids_to_tokens(X_train_xlm['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222d3dac",
   "metadata": {},
   "source": [
    "# class imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a80aed",
   "metadata": {},
   "source": [
    "the distribution of one class is highly skewed so the learning algorithm might \n",
    "tend to be biased towards the majority class leading to poor predictions for the minority class\n",
    "\n",
    " approaches to deal with imbalanced datasets are:\n",
    " \n",
    "- undersample majority class: discards potentially valuable data\n",
    "- oversample minority class: can lead to overfitting and increases training times\n",
    "- weight loss function: assigns higher importance to minority classes providing a direct optimisation approach.\n",
    "sample: wj will be high\n",
    "majority sample: wj will be low\n",
    "- synthetic datasets: could be generated to complement the the minority class and increase its representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c03373",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# resample vectorised x_train and y_train - > returns list of arrays\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.over_sampling import ADASYN # generates synthetic samples in regions of the minority class where the class density is low\n",
    "\n",
    "rus = RandomUnderSampler(random_state=42, sampling_strategy=1)\n",
    "ros = RandomOverSampler(random_state=42, sampling_strategy='not majority')\n",
    "ada = ADASYN(random_state=42)\n",
    "\n",
    "resampler = [rus, ros, ada]\n",
    "\n",
    "# train set\n",
    "\n",
    "res_cv = []\n",
    "for x in resampler:\n",
    "    res_cv.append(x.fit_resample(X_train_cv, np.array(y_train)))\n",
    "    \n",
    "res_tfidf = []\n",
    "for x in resampler:\n",
    "    res_tfidf.append(x.fit_resample(X_train_tfidf, np.array(y_train)))\n",
    "    \n",
    "# oversample embeddings\n",
    "\n",
    "embedding_train_ros, y_train_ros = ros.fit_resample(embedding_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa7215e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(embedding_test_ros)\n",
    "\n",
    "from collections import Counter\n",
    "counter = Counter()\n",
    "\n",
    "for instance_per_class in y_train_ros:\n",
    "    counter[instance_per_class] +=1\n",
    "    \n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a06003",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mpu\n",
    "\n",
    "# baseline\n",
    "\n",
    "mpu.io.write('X_train_cv.pickle', X_train_cv)\n",
    "mpu.io.write('X_test_cv.pickle', X_test_cv)\n",
    "\n",
    "mpu.io.write('X_train_tfidf.pickle', X_train_tfidf)\n",
    "mpu.io.write('X_test_tfidf.pickle', X_test_tfidf)\n",
    "\n",
    "mpu.io.write('embedding_train.pickle', embedding_train)\n",
    "mpu.io.write('embedding_test.pickle', embedding_test)\n",
    "\n",
    "mpu.io.write('y_train.pickle', y_train)\n",
    "mpu.io.write('y_test.pickle', y_test)\n",
    "\n",
    "# resampled\n",
    "\n",
    "mpu.io.write('X_rus_train_tfidf.pickle', res_tfidf[0][0])\n",
    "mpu.io.write('y_rus_train_tfidf.pickle', res_tfidf[0][1])\n",
    "\n",
    "mpu.io.write('X_ros_train_tfidf.pickle', res_tfidf[1][0])\n",
    "mpu.io.write('y_ros_train_tfidf.pickle', res_tfidf[1][1])\n",
    "\n",
    "mpu.io.write('X_ada_train_tfidf.pickle', res_tfidf[2][0])\n",
    "mpu.io.write('y_ada_train_tfidf.pickle', res_tfidf[2][1])\n",
    "\n",
    "mpu.io.write('X_rus_train_cv.pickle', res_cv[0][0])\n",
    "mpu.io.write('y_rus_train_cv.pickle', res_cv[0][1])\n",
    "\n",
    "mpu.io.write('X_ros_train_cv.pickle', res_cv[1][0])\n",
    "mpu.io.write('y_ros_train_cv.pickle', res_cv[1][1])\n",
    "\n",
    "mpu.io.write('X_ada_train_cv.pickle', res_cv[2][0])\n",
    "mpu.io.write('y_ada_train_cv.pickle', res_cv[2][1])\n",
    "\n",
    "mpu.io.write('../data/x_emb_train_ros.pickle', embedding_train_ros) \n",
    "mpu.io.write('../data/y_emb_train_ros.pickle', y_train_ros)\n",
    "\n",
    "# xlm\n",
    "\n",
    "mpu.io.write('X_train_xlm.pickle', X_train_xlm)\n",
    "mpu.io.write('X_test_xlm.pickle', X_test_xlm)\n",
    "mpu.io.write('y_train_xlm.pickle', y_train_xlm)\n",
    "mpu.io.write('y_test_xlm.pickle', y_test_xlm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
