{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# March 2024\n",
    "# Model training - pretrained language model\n",
    "# Violeta Berdejo-Espinola"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device('mps')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mpu \n",
    "\n",
    "x  = mpu.io.read('../data/corpus_raw.pickle')\n",
    "y = [1] * len(x[:62]) + [0] * len(x[62:])\n",
    "\n",
    "pos = x[0:62]\n",
    "neg = x[62:5020]\n",
    "\n",
    "# split data\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=42)\n",
    "\n",
    "# calculate weights for classes\n",
    "\n",
    "class_weights = [(1 - (len(neg) / len(x))), (1 - (len(pos) / len(x)))]\n",
    "class_weights = torch.Tensor(class_weights).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base')\n",
    "     \n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch, padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
    "    # returns dictionary with two key:value. input ids:tensors and attention mask:tensors both of them contain tensors\n",
    "    \n",
    "X_train = tokenize(x_train)                              \n",
    "X_test = tokenize(x_test)\n",
    "\n",
    "y_train = torch.tensor(y_train) \n",
    "y_test = torch.tensor(y_test) \n",
    "\n",
    "X_train['input_ids'].size()\n",
    "X_test['input_ids'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "batch_size = 128 # number of training samples in one forward and backwards pass\n",
    "workers = 0      # how many subprocesses to use for data loading / each worker gets their own subset of indices to construct each batch. they retrieve data and put it into a queue\n",
    "seed = 42\n",
    "G = torch.Generator()\n",
    "G.manual_seed(seed)\n",
    "\n",
    "# function to seed workers in multiprocessing data loading\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "# compile tensorDataset and DataLoaders\n",
    "\n",
    "train_xlm = TensorDataset(X_train['input_ids'], X_train['attention_mask'], y_train)\n",
    "train_dataloader = DataLoader(train_xlm,\n",
    "                              batch_size=batch_size, \n",
    "                              shuffle=False, # set it to false so when dataset is split into batches, this isn't random\n",
    "                              num_workers=workers, \n",
    "                              generator = G, # used to generate random indexes and multiprocessing to generate base seed for workers\n",
    "                              worker_init_fn=seed_worker)\n",
    "\n",
    "\n",
    "test_xlm = TensorDataset(X_test['input_ids'], X_test['attention_mask'], y_test)\n",
    "test_dataloader = DataLoader(test_xlm, \n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=False, \n",
    "                            num_workers=workers, \n",
    "                            generator = G,\n",
    "                            worker_init_fn=seed_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (XLMRobertaForSequenceClassification)\n",
    "\n",
    "id2label = {'negative': 0, 'positive': 1}\n",
    "label2id = {0: 'negative',1: 'positive'}\n",
    "\n",
    "model = XLMRobertaForSequenceClassification.from_pretrained('xlm-roberta-base',\n",
    "                                                            num_labels=2, \n",
    "                                                            id2label=id2label, \n",
    "                                                            label2id=label2id).to(device)\n",
    "\n",
    "# freeze transformer model parametres\n",
    "\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to train model\n",
    "\n",
    "num_epochs = 2\n",
    "learning_rate = 0.1\n",
    "optim = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "criteria = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "\n",
    "def Trainer(dataloader, epochs):\n",
    "\n",
    "    loss_values = []\n",
    " \n",
    "    for epoch in range(epochs):\n",
    "        model.train() # set the model to training mode\n",
    "        running_loss = 0\n",
    "        \n",
    "        print('training on epoch: ', epoch)\n",
    "        \n",
    "        # for step, batch in enumerate(dataloader):\n",
    "        for batch in dataloader:\n",
    "\n",
    "            input_ids = batch[0].to(device)\n",
    "            input_mask = batch[1].to(device)\n",
    "            target = batch[2].to(device)\n",
    "            \n",
    "            # clear gradients\n",
    "            optim.zero_grad() \n",
    "\n",
    "            # forward pass\n",
    "            outputs = model(input_ids,\n",
    "                            token_type_ids=None,\n",
    "                            attention_mask=input_mask)\n",
    "            \n",
    "            # logits from the model's output\n",
    "            logits = outputs.logits\n",
    "            # calculate loss\n",
    "            loss = criteria(logits, target)\n",
    "            # backward pass & compute gradients\n",
    "            loss.backward()\n",
    "            # optimization & update parametres weights\n",
    "            optim.step()\n",
    "            \n",
    "            running_loss += loss.item() \n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss {running_loss / len(dataloader)}\")\n",
    "    \n",
    "        # calculate the average loss over the training data\n",
    "        avg_train_loss = running_loss / len(train_dataloader)\n",
    "        loss_values.append(avg_train_loss)\n",
    "        \n",
    "        print(\"average training loss: {0:.4f}\".format(avg_train_loss))\n",
    "\n",
    "    return loss_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "model_0  = Trainer(train_dataloader, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "def evaluate_model(dataloader):\n",
    "    model.eval()  # set the model to evaluation mode\n",
    "\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():  # disable gradient calculations\n",
    "        for batch in dataloader:\n",
    "            # Transfer batch to the device\n",
    "            input_ids = batch[0].to(device)\n",
    "            attention_mask = batch[1].to(device)\n",
    "            labels = batch[2].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits  # Access the logits directly\n",
    "\n",
    "            # Get the predicted class (e.g., the class with the highest score)\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            \n",
    "            # Append predictions and labels for further processing\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "    return all_predictions, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions, all_labels = evaluate_model(test_dataloader)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "cr = classification_report(all_labels, \n",
    "                           all_predictions, \n",
    "                           output_dict=False)\n",
    "print(cr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
