{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e74ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# February 2024\n",
    "# Resampling and extracting text \n",
    "# Violeta Berdejo-Espinola"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa8000d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import mpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f2bf70",
   "metadata": {},
   "source": [
    "# read data\n",
    "\n",
    "We read two types of data with two different lengths, totalling four corpuses. <br> \n",
    "- title and abstract text with minimal pre-processing (removed only URL) <br>\n",
    "- title abstract and main text text with minimal pre-processing (removed only URL) <br>\n",
    "- title and abstract pre-processed text <br>\n",
    "- title abstract and main re-processed tex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0b39a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean corpus\n",
    "\n",
    "corpus = mpu.io.read('../data/corpus_clean.pickle')\n",
    "corpus_long = mpu.io.read('../data/corpus_clean_long.pickle')\n",
    "\n",
    "x = corpus\n",
    "x_long = corpus_long\n",
    "y = [1] * len(corpus[:62]) + [0] * len(corpus[62:])\n",
    "\n",
    "# raw corpus\n",
    "\n",
    "corpus_raw = mpu.io.read('../data/corpus_raw.pickle')\n",
    "corpus_raw_long = mpu.io.read('../data/corpus_raw_long.pickle')\n",
    "\n",
    "x_raw = corpus_raw\n",
    "x_raw_long = corpus_raw_long"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023796ad",
   "metadata": {},
   "source": [
    "# split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b26f13f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 3966, 1: 49})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=42)\n",
    "x_train_long, x_test_long, y_train_long, y_test_long = train_test_split(x_long, y, test_size=0.20, random_state=42)\n",
    "x_train_r, x_test_r, y_train_r, y_test_r = train_test_split(x_raw, y, test_size=0.20, random_state=42)\n",
    "x_train_r_long, x_test_r_long, y_train_r_long, y_test_r_long = train_test_split(x_raw_long, y, test_size=0.20, random_state=42)\n",
    "\n",
    "# check instances in each class\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "cntt = Counter()\n",
    "\n",
    "for instance_per_class in y_train_long:\n",
    "    cntt[instance_per_class] += 1\n",
    "\n",
    "cntt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf48735c",
   "metadata": {},
   "source": [
    "# feature extraction\n",
    "\n",
    "https://stackoverflow.com/questions/62812198/valueerror-in-while-predict-where-test-data-is-having-different-shape-of-word-ve\n",
    "\n",
    "transformer tokenizer: https://huggingface.co/transformers/v2.11.0/main_classes/tokenizer.html#:~:text=max_length%20\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57900100",
   "metadata": {},
   "source": [
    "# count-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66bb740b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer # uses one-dim array of strings ~ shape (n,)\n",
    "from sklearn.feature_extraction.text import CountVectorizer # returns arrays\n",
    "\n",
    "vect_tfidf = TfidfVectorizer()\n",
    "vect_cv = CountVectorizer()\n",
    "\n",
    "# corpus with title and abstract\n",
    "\n",
    "X_train_cv = vect_cv.fit_transform(x_train)                    #fit: tokenize & buid vocab (turn object into an estimator) model learns the vectors to which they are used to transform data\n",
    "X_train_tfidf = vect_tfidf.fit_transform(x_train)              #transform: transforms instances into matrices\n",
    "X_test_cv = vect_cv.transform(x_test)\n",
    "X_test_tfidf = vect_tfidf.transform(x_test) \n",
    "\n",
    "import mpu\n",
    "\n",
    "mpu.io.write('X_train_cv.pickle', X_train_cv)\n",
    "mpu.io.write('X_test_cv.pickle', X_test_cv)\n",
    "\n",
    "mpu.io.write('X_train_tfidf.pickle', X_train_tfidf)\n",
    "mpu.io.write('X_test_tfidf.pickle', X_test_tfidf)\n",
    "\n",
    "mpu.io.write('y_train.pickle', y_train)\n",
    "mpu.io.write('y_test.pickle', y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7de7695f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
       "\twith 598947 stored elements and shape (1004, 162943)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# corpus with title abstract and main text \n",
    "\n",
    "X_train_cv = vect_cv.fit_transform(x_train_long)                    #fit: tokenize & buid vocab (turn object into an estimator)\n",
    "X_train_tfidf = vect_tfidf.fit_transform(x_train_long)              #transform: instances into matrices\n",
    "X_test_cv = vect_cv.transform(x_test_long)\n",
    "X_test_tfidf = vect_tfidf.transform(x_test_long) \n",
    "\n",
    "mpu.io.write('X_train_long_cv.pickle', X_train_cv)\n",
    "mpu.io.write('X_test_long_cv.pickle', X_test_cv)\n",
    "\n",
    "mpu.io.write('X_train_long_tfidf.pickle', X_train_tfidf)\n",
    "mpu.io.write('X_test_long_tfidf.pickle', X_test_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f97b10",
   "metadata": {},
   "source": [
    "# embedding text\n",
    "\n",
    "output are dense vector that encapsulate semantic meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89ddbb2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/uqvberde/Projects/classifier_spanish/.env/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0\n",
      "('14.7', ('', '', ''), 'arm64')\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import platform\n",
    "\n",
    "print(torch.__version__) # checking pytorch version 2.0 or more\n",
    "print(platform.mac_ver()) # checking pytorch version for mac - should be arm64\n",
    "print(torch.backends.mps.is_built())  # checking if mps is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c64c7958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00914845, -0.00346731,  0.01054023, ...,  0.00480202,\n",
       "         0.03056456,  0.10237923],\n",
       "       [ 0.01896946,  0.0260251 , -0.00869109, ..., -0.0671379 ,\n",
       "         0.03614533, -0.01881293],\n",
       "       [ 0.00585423, -0.06012807, -0.03169392, ...,  0.00432672,\n",
       "         0.05887518, -0.05666463],\n",
       "       ...,\n",
       "       [-0.00157769,  0.08838714,  0.00691193, ..., -0.04349812,\n",
       "         0.00940177, -0.00767317],\n",
       "       [-0.00077686, -0.0094168 ,  0.01987838, ...,  0.00946549,\n",
       "         0.02030043,  0.05779348],\n",
       "       [-0.02183528,  0.01464673,  0.07375681, ...,  0.01719196,\n",
       "         0.0200747 , -0.02840189]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_distil = SentenceTransformer('distiluse-base-multilingual-cased-v1') \n",
    "\n",
    "distil_train = model_distil.encode(x_train_r_long)\n",
    "distil_test = model_distil.encode(x_test_r_long)\n",
    "\n",
    "mpu.io.write('../data/embedding_train_long.pickle', distil_train)\n",
    "mpu.io.write('../data/embedding_test_long.pickle', distil_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c66c5cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:32<00:00,  8.14s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.254  ,  0.54   , -2.879  , ..., -0.03717, -0.5786 ,  0.6885 ],\n",
       "       [-3.59   ,  4.508  , -1.722  , ..., -0.997  ,  3.465  , -0.9683 ],\n",
       "       [-0.8643 ,  3.86   ,  4.34   , ..., -0.6377 ,  3.936  , -3.604  ],\n",
       "       ...,\n",
       "       [-2.873  , -1.105  , -1.051  , ..., -2.984  ,  0.1659 ,  2.074  ],\n",
       "       [ 0.4363 ,  2.512  ,  2.988  , ...,  2.98   ,  1.547  , -1.72   ],\n",
       "       [ 1.334  ,  1.533  ,  1.433  , ...,  1.312  ,  5.16   , -2.514  ]],\n",
       "      dtype=float16)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_gemma2 = SentenceTransformer(\"BAAI/bge-multilingual-gemma2\", \n",
    "                                    model_kwargs={\"torch_dtype\": torch.float16},\n",
    "                                    device=torch.device('mps'))\n",
    "\n",
    "gemma2_train = model_gemma2.encode(x_train_r_long)\n",
    "gemma2_test = model_gemma2.encode(x_test_r_long)\n",
    "\n",
    "mpu.io.write('../data/gemma2_train_long.pickle', gemma2_train)\n",
    "mpu.io.write('../data/gemma2_test_long.pickle', gemma2_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6826da04",
   "metadata": {},
   "source": [
    "# pre-trained model as a feature extractor\n",
    "output is a sequence of token IDs and attention masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f90ae79-c9ea-4318-97d0-e041e33e1576",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base')\n",
    "     \n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch, padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
    "    # returns dictionary with two key:value. input ids:tensors and attention mask:tensors both of them contain tensors\n",
    "    \n",
    "X_train_xlm = tokenize(x_train_r)                              \n",
    "X_test_xlm = tokenize(x_test_r)\n",
    "\n",
    "y_train_xlm = torch.tensor(y_train_r) \n",
    "y_test_xlm = torch.tensor(y_test_r) \n",
    "\n",
    "a = X_train_xlm['input_ids'].size()\n",
    "b = X_test_xlm['input_ids'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db567890",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpu.io.write('X_train_xlm.pickle', X_train_xlm)\n",
    "mpu.io.write('X_test_xlm.pickle', X_test_xlm)\n",
    "mpu.io.write('y_train_xlm.pickle', y_train_xlm)\n",
    "mpu.io.write('y_test_xlm.pickle', y_test_xlm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce3f9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('document-term matrix\\n')\n",
    "print(f'count based vectors - cv & tfidf:\\n {X_train_cv.shape, X_test_cv.shape} \\n {X_train_tfidf.shape, X_test_tfidf.shape}\\n')\n",
    "print(f'embedding - sentence transformer:\\n {embedding_train.shape, embedding_test.shape}\\n')\n",
    "print(f'embedding - xlm roberta:\\n {a} {b}\\n') # size is batch_size, n_tokens\n",
    "\n",
    "# transformers \n",
    "\n",
    "print(torch.is_tensor(X_train_xlm))\n",
    "print(type(X_train_xlm))\n",
    "\n",
    "print(f'xlm vocabulary size: {tokenizer.vocab_size} \\nmodel context size: {tokenizer.model_max_length}\\nmodel input {tokenizer.model_input_names}\\n')\n",
    "print(f'xlm input ids:\\n {X_train_xlm.input_ids}\\nxlm attention masks:\\n {X_train_xlm.attention_mask}\\n')\n",
    "\n",
    "print('let\\'s explore an example:\\n')\n",
    "print(X_train_xlm['input_ids'][10].size())\n",
    "\n",
    "# the input sequence of each batch is padded [1] to the maximum sequence length in the batch (model context size)\n",
    "# the attention mask array is used to ignore the paddded areas of the betch \n",
    "print(X_train_xlm['input_ids'][0])\n",
    "print(tokenizer.convert_ids_to_tokens(X_train_xlm['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222d3dac",
   "metadata": {},
   "source": [
    "# dealing with class imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a80aed",
   "metadata": {},
   "source": [
    "the distribution of one class is highly skewed so the learning algorithm might \n",
    "tend to be biased towards the majority class leading to poor predictions for the minority class\n",
    "\n",
    " approaches to deal with imbalanced datasets are:\n",
    " \n",
    "- undersample majority class: discards potentially valuable data\n",
    "- oversample minority class: can lead to overfitting and increases training times\n",
    "- weight loss function: assigns higher importance to minority classes providing a direct optimisation approach.\n",
    "sample: wj will be high\n",
    "majority sample: wj will be low\n",
    "- synthetic datasets: could be generated to complement the the minority class and increase its representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c03373",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# resample vectorised x_train and y_train - > returns list of arrays\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.over_sampling import ADASYN # generates synthetic samples in regions of the minority class where the class density is low\n",
    "\n",
    "rus = RandomUnderSampler(random_state=42, sampling_strategy=1)\n",
    "ros = RandomOverSampler(random_state=42, sampling_strategy='not majority')\n",
    "ada = ADASYN(random_state=42)\n",
    "\n",
    "resampler = [rus, ros, ada]\n",
    "\n",
    "# train set\n",
    "\n",
    "res_cv = []\n",
    "for x in resampler:\n",
    "    res_cv.append(x.fit_resample(X_train_cv, np.array(y_train)))\n",
    "    \n",
    "res_tfidf = []\n",
    "for x in resampler:\n",
    "    res_tfidf.append(x.fit_resample(X_train_tfidf, np.array(y_train)))\n",
    "    \n",
    "# oversample embeddings\n",
    "\n",
    "embedding_train_ros, y_train_ros = ros.fit_resample(distil_train, y_train)\n",
    "\n",
    "# checking number of instances per class\n",
    "\n",
    "len(embedding_test_ros)\n",
    "\n",
    "from collections import Counter\n",
    "counter = Counter()\n",
    "\n",
    "for instance_per_class in y_train_ros:\n",
    "    counter[instance_per_class] +=1\n",
    "    \n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a06003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resampled\n",
    "\n",
    "mpu.io.write('X_rus_train_tfidf.pickle', res_tfidf[0][0])\n",
    "mpu.io.write('y_rus_train_tfidf.pickle', res_tfidf[0][1])\n",
    "\n",
    "mpu.io.write('X_ros_train_tfidf.pickle', res_tfidf[1][0])\n",
    "mpu.io.write('y_ros_train_tfidf.pickle', res_tfidf[1][1])\n",
    "\n",
    "mpu.io.write('X_ada_train_tfidf.pickle', res_tfidf[2][0])\n",
    "mpu.io.write('y_ada_train_tfidf.pickle', res_tfidf[2][1])\n",
    "\n",
    "mpu.io.write('X_rus_train_cv.pickle', res_cv[0][0])\n",
    "mpu.io.write('y_rus_train_cv.pickle', res_cv[0][1])\n",
    "\n",
    "mpu.io.write('X_ros_train_cv.pickle', res_cv[1][0])\n",
    "mpu.io.write('y_ros_train_cv.pickle', res_cv[1][1])\n",
    "\n",
    "mpu.io.write('X_ada_train_cv.pickle', res_cv[2][0])\n",
    "mpu.io.write('y_ada_train_cv.pickle', res_cv[2][1])\n",
    "\n",
    "mpu.io.write('../data/x_emb_train_ros.pickle', embedding_train_ros) \n",
    "mpu.io.write('../data/y_emb_train_ros.pickle', y_train_ros)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
