{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e74ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# November 2022\n",
    "# Feature engineering\n",
    "# Violeta Berdejo-Espinola"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c2e59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy pandas spacy mpu spacy stop-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab76f973",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2b98f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/pos_neg.csv', encoding='utf-8')\n",
    "\n",
    "print(f'duplicates: {df.title_spa.duplicated().any()}')\n",
    "print(f'nas: {df[\"label\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db39bb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x data\n",
    "\n",
    "corpus_list = df.loc[:,\"title_spa\":\"abstract_spa\"].values.tolist()\n",
    "corpus_list_long = df.loc[:,\"title_spa\":\"main_text_spa\"].values.tolist()\n",
    "\n",
    "print(f'instances per\\n{df[\"label\"].value_counts()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936d1283",
   "metadata": {},
   "source": [
    "# feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137fad3f",
   "metadata": {},
   "source": [
    "# removing special characters, punctiation, and numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ea205b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to enact regex substitution on a list of strings\n",
    "\n",
    "def sub_all(regex, corpus_list, replacement=\" \"):\n",
    "    \n",
    "    return [[regex.sub(replacement, col) for col in row] for row in corpus_list]\n",
    "\n",
    "# defining regular expressions as objects to find unwated text and symbols in corpus\n",
    "\n",
    "re_citation = re.compile(r\"\\(.[^())]*\\d{4}[^())]*\\)\")\n",
    "re_tabfig = re.compile(r\"\\(\\s?\\w{1,7}[.]?\\s?\\d{1}\\w?\\s?\\)\")\n",
    "re_digit_char = re.compile(r\"\\d+\\w{,2}\")\n",
    "re_one_two_letter = re.compile(r\"\\b\\w{1,2}\\b\")\n",
    "re_new_line = re.compile(r\"\\n{1,}\")\n",
    "re_tab = re.compile(r\"\\t{1,}\")\n",
    "re_html = re.compile(r\"</?\\w+>\")\n",
    "re_alt_html = re.compile(r\"<.*?>\")\n",
    "re_spacing = re.compile(r\"\\s{2,}\")\n",
    "re_fig = re.compile(r\"(fig)\")\n",
    "re_table = re.compile(r\"(cuadro)\")\n",
    "punctuation_text = string.punctuation + \"¿±♂♀’”°´“×–…\" + \"\\xad\" + \"\\xa0\"\n",
    "translator = str.maketrans(punctuation_text, \" \" * len(punctuation_text))\n",
    "\n",
    "# function to process text and output 'clean corpus'\n",
    "\n",
    "def text_processing(corpus_list):\n",
    "    \n",
    "    output = [\n",
    "        [col.lower() if type(col) is str else \"\" for col in row] for row in corpus_list\n",
    "    ]\n",
    "    output = sub_all(re_citation, output)\n",
    "    output = sub_all(re_tabfig, output)\n",
    "    output = sub_all(re_fig, output)\n",
    "    output = sub_all(re_table, output)\n",
    "    output = sub_all(re_digit_char, output)\n",
    "    output = sub_all(re_one_two_letter, output)\n",
    "    output = [[col.translate(translator) for col in row] for row in output]\n",
    "#     output = sub_all(re_non_breaking_space, output)\n",
    "    output = sub_all(re_new_line, output)\n",
    "    output = sub_all(re_tab, output)\n",
    "    output = sub_all(re_html, output)\n",
    "    output = sub_all(re_alt_html, output)\n",
    "    output = sub_all(re_spacing, [[word.strip() for word in row] for row in output])\n",
    "\n",
    "    return output\n",
    "\n",
    "# function to process text and output 'raw corpus'\n",
    "\n",
    "def text_processing_raw(text):\n",
    "\n",
    "    output = sub_all(re_html, text)\n",
    "    output = sub_all(re_alt_html, output)\n",
    "    \n",
    "    return output\n",
    "\n",
    "# run\n",
    "\n",
    "corpus_clean = text_processing(corpus_list)\n",
    "corpus_clean_long = text_processing(corpus_list_long)\n",
    "\n",
    "corpus_clean_raw = text_processing_raw(corpus_list)\n",
    "corpus_clean_raw_long = text_processing_raw(corpus_list_long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8063e38c",
   "metadata": {},
   "source": [
    "# lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074c4d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "MODEL = 'es_core_news_md'\n",
    "spacy.cli.download(MODEL) \n",
    "nlp = spacy.load(MODEL, disable=['parser', 'ner'])\n",
    "\n",
    "def lemmatizer(text):\n",
    "    \n",
    "    doc_list = []\n",
    "    for sentence in text: \n",
    "        doc_list.append(\" \".join([token.lemma_ for token in nlp(\" \".join(sentence))]))\n",
    "    \n",
    "    return doc_list\n",
    "\n",
    "corpus_clean = lemmatizer(corpus_clean)\n",
    "corpus_clean_long = lemmatizer(corpus_clean_long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8219b239",
   "metadata": {},
   "source": [
    "# removing stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8450aa11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stop_words import get_stop_words\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    \n",
    "    corpus_clean = [\n",
    "    \" \".join([word for word in sentence.split() if re.sub(r'\\W+', '', word) not in get_stop_words('spanish')]) for sentence in text\n",
    "]\n",
    "    if any (stopword in corpus_clean for stopword in get_stop_words('spanish')):\n",
    "        print ('stopwords not excluded from vocabulary')\n",
    "    else:\n",
    "        print ('stopwords excluded from vocabulary')\n",
    "    if any (number in corpus_clean for number in list(range(1,1000001))):\n",
    "        print ('\\nnumbers not excluded from vocabulary')\n",
    "    else:\n",
    "        print ('\\nnumbers excluded from vocabulary')\n",
    "\n",
    "    return corpus_clean\n",
    "\n",
    "corpus_clean = remove_stopwords(corpus_clean)\n",
    "corpus_clean_long = remove_stopwords(corpus_clean_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689dd7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# character length of each example before and after text preprocessing\n",
    "\n",
    "each_example_len_1 = []\n",
    "for each_example in corpus_list:\n",
    "    each_example_len_1.append(sum(map(len, each_example)))\n",
    "\n",
    "each_example_len_2 = []\n",
    "for each_example in corpus_clean:\n",
    "    each_example_len_2.append(len(each_example))\n",
    "\n",
    "lens = pd.DataFrame({\"len_before_processing\":each_example_len_1,\n",
    "                    \"len_after_processing\":each_example_len_2})\n",
    "lens\n",
    "\n",
    "lens.to_csv('../results/preprocessing/2.diff_word_length_after_feat_eng_longcorpus.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84e0d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating lists of pos and neg instances\n",
    "\n",
    "pos = corpus_clean[0:62]\n",
    "neg = corpus_clean[62:5020]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09282ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data to disk - serialise python object to bytes\n",
    "\n",
    "import mpu\n",
    "\n",
    "mpu.io.write('../data/neg.pickle', neg)\n",
    "mpu.io.write('../data/pos.pickle', pos)\n",
    "mpu.io.write(\"../data/corpus_clean.pickle\", corpus_clean)\n",
    "mpu.io.write(\"../data/corpus_clean_long.pickle\", corpus_clean_long)\n",
    "mpu.io.write(\"../data/corpus_raw_long.pickle\", corpus_clean_raw_long)\n",
    "mpu.io.write(\"../data/corpus_raw.pickle\", corpus_clean_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0628c4ff",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
