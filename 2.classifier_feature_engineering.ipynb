{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e74ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# November 2022\n",
    "# Text classifier using logistic regression\n",
    "# Violeta Berdejo-Espinola"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8000d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# linting\n",
    "# !nbqa pylint 1.pre_process_main_text.ipynb\n",
    "\n",
    "# background theme \n",
    "# !jt -t monokai -cellw 90% #grade3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab76f973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import display, HTML\n",
    "# display(HTML(\"<style>.container { width:90% !important; }</style>\"))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "\n",
    "pd.options.display.max_columns = 65\n",
    "# os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e2b98f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../datasets/py_outputs/pos_neg.csv', encoding='utf-8')\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "labeler = LabelEncoder()\n",
    "labeler.fit([0,1])\n",
    "\n",
    "df[\"label\"] = labeler.fit_transform(df[\"label\"])\n",
    "\n",
    "df['abstract_spa'] = df['abstract_spa'].str.replace(r'Resumen', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abad9e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "counter= Counter()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db39bb5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of article list is 5019\n"
     ]
    }
   ],
   "source": [
    "# x data\n",
    "\n",
    "corpus_df = df.loc[:,\"title_spa\":\"abstract_spa\"]\n",
    "corpus_list = corpus_df.values.tolist()\n",
    "\n",
    "print(f'length of article list is {len(corpus_list)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c35c23f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5019\n",
      "False\n",
      "0    4957\n",
      "1      62\n",
      "Name: label, dtype: int64\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['Manejo forestal comunitario en el sur de méxico: ¿es una práctica sustentable para el mantenimiento de los ensambles de escarabajos?',\n",
       "  'Evaluamos los cambios en la diversidad, estructura y composición de especies de escarabajos copronecrófagos (Coleoptera: Scarabaeidae: Scarabaeinae) en un paisaje bajo manejo forestal comunitario en el sur de México. Se dispusieron trampas de caída cebadas con excretas de cerdo y calamar en descomposición en sitios con diferente tratamiento forestal. En total, se registraron 3,608 individuos y 21 especies de Scarabaeinae. Registramos un mayor número de especies en el área sin intervención y cambios significativos en la composición de especies entre sitios. Dos escarabajos generalistas (Ontherus mexicanus y Onthophagus cyanellus) representaron 51% de la abundancia total. Nuestros resultados indican que el efecto relativo del manejo forestal sobre el ensamble de escarabajos es proporcional a la intensidad de corte. El impacto negativo sobre las comunidades de escarabajos puede contravenir al modelo de aprovechamiento silvícola, pues la pérdida gradual de especies a nivel local podría generar impactos significativos en la funcionalidad del ecosistema en el largo plazo. Es necesario establecer esquemas de monitoreo para la valoración del estatus de los elementos de la biodiversidad, con el fin de garantizar un manejo comunitario sustentable en áreas forestales en el Sur de México.'],\n",
       " ['La conservación de mamíferos medianos en dos reservas ecológicas privadas de veracruz, méxico',\n",
       "  ' La fauna es un elemento particularmente distintivo y carismático de la biodiversidad tropical que está siendo sometido a una amenaza sin precedentes por la cacería furtiva y la pérdida de su hábitat. Este fenómeno denominado defaunación impacta de manera directa la biodiversidad tropical al extirpar poblaciones de animales, reducir su variabilidad genética y en el extremo llevar a la extinción completa de especies. Las Reservas privadas como Santa Gertrudis (SG) en Vega de Alatorre y La Otra Opción (LOO) en Los Tuxtlas, Veracruz, están haciendo esfuerzos por conservar la biodiversidad de la fauna. En 2014 y 2015 se utilizaron cámaras-trampa para registrar la fauna de mamíferos con un esfuerzo de muestreo en SG de 531 días/cámara en 2014 y 1,093 en 2015, mientras que en LOO en 2014 fueron 1,090 d/c, y en 2015, 1,081. En SG las especies más comunes fueron: Dasypus novemcinctus, Didelphis marsupialis y Cuniculus paca, y en LOO, Dasypus novemcinctus, Sciurus aureogaster y Dasyprocta mexicana. Se registraron especies en riesgo de extinción como Galictis vittata (SG); mientras que en LOO: Leopardus wiedii, Herpailurus yagouaroundi y Eira barbara.']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(df))\n",
    "print(df.title_spa.duplicated().any())\n",
    "print(df[\"label\"].value_counts())\n",
    "print(df[\"label\"].isna().sum())\n",
    "df.iloc[0:62]\n",
    "df.head(63)\n",
    "corpus_list[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936d1283",
   "metadata": {},
   "source": [
    "# feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137fad3f",
   "metadata": {},
   "source": [
    "# removing special characters, punctiation, and numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ea205b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# function to enact regex substitution on a list of strings\n",
    "\n",
    "def sub_all(regex, corpus_list, replacement=\" \"):\n",
    "    \n",
    "    return [[regex.sub(replacement, col) for col in row] for row in corpus_list]\n",
    "\n",
    "# defining regular expressions as objects to find unwated text and symbols in corpus\n",
    "\n",
    "re_citation = re.compile(r\"\\(.[^())]*\\d{4}[^())]*\\)\")\n",
    "re_tabfig = re.compile(r\"\\(\\s?\\w{1,7}[.]?\\s?\\d{1}\\w?\\s?\\)\")\n",
    "re_digit_char = re.compile(r\"\\d+\\w{,2}\")\n",
    "re_one_two_letter = re.compile(r\"\\b\\w{1,2}\\b\")\n",
    "re_new_line = re.compile(r\"\\n{1,}\")\n",
    "re_tab = re.compile(r\"\\t{1,}\")\n",
    "re_html = re.compile(r\"</?\\w+>\")\n",
    "re_alt_html = re.compile(r\"<.*?>\")\n",
    "re_spacing = re.compile(r\"\\s{2,}\")\n",
    "re_fig = re.compile(r\"(fig)\")\n",
    "re_table = re.compile(r\"(cuadro)\")\n",
    "punctuation_text = string.punctuation + \"¿±♂♀’”°´“×–…\" + \"\\xad\" + \"\\xa0\"\n",
    "translator = str.maketrans(punctuation_text, \" \" * len(punctuation_text))\n",
    "\n",
    "# function to process text\n",
    "\n",
    "def text_processing(corpus_list):\n",
    "    \n",
    "    output = [\n",
    "        [col.lower() if type(col) is str else \"\" for col in row] for row in corpus_list\n",
    "    ]\n",
    "    output = sub_all(re_citation, output)\n",
    "    output = sub_all(re_tabfig, output)\n",
    "    output = sub_all(re_fig, output)\n",
    "    output = sub_all(re_table, output)\n",
    "    output = sub_all(re_digit_char, output)\n",
    "    output = sub_all(re_one_two_letter, output)\n",
    "    output = [[col.translate(translator) for col in row] for row in output]\n",
    "#     output = sub_all(re_non_breaking_space, output)\n",
    "    output = sub_all(re_new_line, output)\n",
    "    output = sub_all(re_tab, output)\n",
    "    output = sub_all(re_html, output)\n",
    "    output = sub_all(re_alt_html, output)\n",
    "    output = sub_all(re_spacing, [[word.strip() for word in row] for row in output])\n",
    "\n",
    "    return output\n",
    "\n",
    "corpus_clean1 = text_processing(corpus_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895eeb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_clean1[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8063e38c",
   "metadata": {},
   "source": [
    "# lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074c4d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('es_core_news_md', disable=['parser', 'ner']) # pre-trained spacy Spanish language object #!python3 -m spacy download es_core_news_md \n",
    "\n",
    "def lemmatizer(text):\n",
    "    \n",
    "    doc_list = []\n",
    "    for sentence in text: \n",
    "        doc_list.append(\" \".join([token.lemma_ for token in nlp(\" \".join(sentence))]))\n",
    "    \n",
    "    return doc_list\n",
    "\n",
    "corpus_clean2 = lemmatizer(corpus_clean1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fca77f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_clean2[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8219b239",
   "metadata": {},
   "source": [
    "# removing stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8450aa11",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from stop_words import get_stop_words\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    \n",
    "    corpus_clean = [\n",
    "    \" \".join([word for word in sentence.split() if re.sub(r'\\W+', '', word) not in get_stop_words('spanish')]) for sentence in text\n",
    "]\n",
    "    if any (stopword in corpus_clean for stopword in get_stop_words('spanish')):\n",
    "        print ('stopwords not excluded from vocabulary')\n",
    "    else:\n",
    "        print ('stopwords excluded from vocabulary')\n",
    "    if any (number in corpus_clean for number in list(range(1,1000001))):\n",
    "        print ('\\nnumbers not excluded from vocabulary')\n",
    "    else:\n",
    "        print ('\\nnumbers excluded from vocabulary')\n",
    "\n",
    "    return corpus_clean\n",
    "\n",
    "corpus_clean3 = remove_stopwords(corpus_clean2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0c5b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_clean3[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689dd7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# character length of each example before and after text preprocessing\n",
    "\n",
    "each_example_len_1 = []\n",
    "for each_example in doc_list:\n",
    "    each_example_len_1.append(sum(map(len, each_example)))\n",
    "\n",
    "each_example_len_2 = []\n",
    "for each_example in corpus_clean:\n",
    "    each_example_len_2.append(len(each_example))\n",
    "\n",
    "lens = pd.DataFrame({\"len_before_processing\":each_example_len_1,\n",
    "                    \"len_after_processing\":each_example_len_2})\n",
    "lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09282ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data to disk - serialise python object to bytes\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('corpus_clean.pickle', 'wb') as handle:\n",
    "    pickle.dump(corpus_clean3, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913ddb07",
   "metadata": {},
   "source": [
    "# keyword-based classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c4d80a",
   "metadata": {},
   "source": [
    "Here, we build a keyword based classifier to identify the most frequent words in positive instances under the assumption that these are a predictors of the positive class. Thus, we remove the negative instances that have those words present in the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76239bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find frequent words in positive instances\n",
    "\n",
    "pos = corpus_clean3[0:62]\n",
    "neg = corpus_clean3[62:5020]\n",
    "neg_complete = neg.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3e09e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8665d19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to count frequency of words\n",
    "\n",
    "from nltk import ngrams\n",
    "\n",
    "n = 2\n",
    "def count_word(text):\n",
    "    \n",
    "    wordcount = {}\n",
    "    for each_example in text:\n",
    "        for gram in ngrams(each_example.split(), n):\n",
    "            if gram not in wordcount:\n",
    "                wordcount[gram] = 1\n",
    "            else:\n",
    "                wordcount[gram] += 1\n",
    "            \n",
    "    return wordcount\n",
    "\n",
    "# count word frequency and find indices of instances\n",
    "\n",
    "word_count = count_word(pos)\n",
    "word_count_sorted = sorted(word_count.items(), key = lambda item:item[1], reverse=True)\n",
    "common_gram_tuple = word_count_sorted[:50]\n",
    "common_gram = [word[0] for word in common_gram_tuple]\n",
    "\n",
    "top_grams = []\n",
    "for gram in common_gram:\n",
    "    top_grams.append(\" \".join(gram))\n",
    "        \n",
    "print(top_grams)\n",
    "len(top_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481e22a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove grams that are verbs, places, etc\n",
    "\n",
    "grams_remove = ['poder ser', 'haber ser', 'ser mayor', 'objetivo ser', 'costa rico', 'robinson crusoe', 'sur méxico', 'encontrar diferencia', 'diferencia significativo', 'ser determinar', 'resultado mostrar', 'ser significativamente', 'ser menor', 'san josé', 'haber pasar','presentar mayor']\n",
    "\n",
    "indices = []\n",
    "for i, item in enumerate(top_grams):\n",
    "    if item in grams_remove:\n",
    "        indices.append(i)\n",
    "        \n",
    "for indx in sorted(indices, reverse = True): \n",
    "    del top_grams[indx]\n",
    "\n",
    "print(indices)\n",
    "print(len(top_grams))\n",
    "top_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0736e97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to find indices of instances that use frequent words\n",
    "\n",
    "def find_indices(list1, list2, min_word_count=2):\n",
    "    \n",
    "    indices = []\n",
    "    for i, item1 in enumerate(list1):\n",
    "        word_count = sum(1 for gram in list2 if gram in item1)\n",
    "        if word_count >= min_word_count:\n",
    "            indices.append(i)\n",
    "            \n",
    "    return indices\n",
    "\n",
    "indices_pos = find_indices(pos, top_grams)\n",
    "indices_neg = find_indices(neg, top_grams)\n",
    "\n",
    "print(len(neg_complete))\n",
    "print(len(indices_pos))#62\n",
    "print(len(indices_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4813f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot wordcloud of vocabulary in each corpus\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "# pip install Pillow --> this library is needed to read in image as the mask for the word cloud\n",
    "from PIL import Image \n",
    "\n",
    "pos_words = \" \".join(pos)\n",
    "neg_words = \" \".join(neg)\n",
    "\n",
    "stopwords = set(STOPWORDS) # excludes stopwords in plot\n",
    "\n",
    "def plot_wordcloud(text):\n",
    "    \n",
    "    # create wordcloud object\n",
    "    \n",
    "    wordcloud = WordCloud(\n",
    "    width = 800, height = 800,\n",
    "    background_color ='white',\n",
    "    stopwords = stopwords,\n",
    "    min_font_size = 10).generate(text)\n",
    "    \n",
    "    # plot the WordCloud image\n",
    "    \n",
    "    plt.figure(figsize = (8, 8), facecolor = None)\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout(pad = 0)\n",
    "    plt.show()\n",
    "\n",
    "# wordcloud.to_file(\"name.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68efa30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wordcloud(pos_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd05b442",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wordcloud(neg_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4391bc2",
   "metadata": {},
   "source": [
    "# removing instances from negative corpus that contain frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0227a572",
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_examples = []\n",
    "for indx in sorted(indices_neg, reverse=True):\n",
    "    if indx < len(neg):\n",
    "        removed_examples.append(neg.pop(indx))\n",
    "        \n",
    "print(len(removed_examples))\n",
    "removed_examples[:]\n",
    "\n",
    "print(len(neg_complete))\n",
    "print(len(neg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a96aa6a",
   "metadata": {},
   "source": [
    "By the end of the pre-processing section we have:\n",
    "\n",
    "- one corpus of positives with lenght 62\n",
    "- two corpuses of negatives:\n",
    "    - neg_complete with length 4957\n",
    "    - neg with length 3848"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e713436d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mpu\n",
    "\n",
    "mpu.io.write('neg_short.pickle', neg)\n",
    "mpu.io.write('neg_complete.pickle', neg_complete)\n",
    "mpu.io.write('pos.pickle', pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0628c4ff",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
