{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e74ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# February 2024\n",
    "# Text classifier using logistic regression, support vector machine, and xlm-roberta\n",
    "# Violeta Berdejo-Espinola"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa8000d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# linting\n",
    "# !nbqa pylint 1.pre_process_main_text.ipynb\n",
    "\n",
    "# background theme \n",
    "# !jt -t monokai -cellw 90% #grade3\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import mpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0b39a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = mpu.io.read('corpus_clean.pickle')\n",
    "pos = mpu.io.read('pos.pickle')\n",
    "neg = mpu.io.read('neg_complete.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "213b32f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5019\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4957"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = pos + neg\n",
    "y = [1] * len(pos) + [0] * len(neg)\n",
    "\n",
    "print(len(y))\n",
    "len(neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6f2564",
   "metadata": {},
   "source": [
    "# splitting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b26f13f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 3966, 1: 49})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=42)\n",
    "\n",
    "cntt = Counter()\n",
    "\n",
    "for bin_class in y_train:\n",
    "    cntt[bin_class] += 1\n",
    "\n",
    "cntt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb66892a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return text with max length\n",
    "\n",
    "def find_max_length(lst):\n",
    "    maxList = max(lst, key=len)\n",
    "    maxLength = len(maxList)\n",
    "     \n",
    "    return maxList, maxLength\n",
    "\n",
    "find_max_length(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf48735c",
   "metadata": {},
   "source": [
    "# feature extraction\n",
    "\n",
    "https://stackoverflow.com/questions/62812198/valueerror-in-while-predict-where-test-data-is-having-different-shape-of-word-ve\n",
    "\n",
    "trabsformer tokenizer: Tokenizing (splitting strings in sub-word token strings), converting tokens strings to ids and back, and encoding/decoding (i.e., tokenizing and converting to integers).\n",
    "Adding new tokens to the vocabulary in a way that is independent of the underlying structure (BPE, SentencePiece…).\n",
    "Managing special tokens (like mask, beginning-of-sentence, etc.): adding them, assigning them to attributes in the tokenizer for easy access and making sure they are not split during tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "66bb740b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xlm vocabulary size: 250002 \n",
      "model context size: 512, ['input_ids', 'attention_mask']\n",
      "CPU times: user 9.39 s, sys: 886 ms, total: 10.3 s\n",
      "Wall time: 22.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# vectorise data using frquency counters and transformer embeddings - > return arrays and dict of tensors\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer # uses one-dim array of strings ~ shape (n,)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vect_tfidf = TfidfVectorizer()\n",
    "vect_cv = CountVectorizer()\n",
    "\n",
    "X_train_cv = vect_cv.fit_transform(x_train)                    #fit: tokenize & buid vocab (turn object into an estimator)\n",
    "X_train_tfidf = vect_tfidf.fit_transform(x_train)              #transform: instances into matrices\n",
    "X_test_cv = vect_cv.transform(x_test)\n",
    "X_test_tfidf = vect_tfidf.transform(x_test) \n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embed_model = SentenceTransformer('distiluse-base-multilingual-cased-v1') \n",
    "\n",
    "embedding_train = embed_model.encode(x_train)\n",
    "embedding_test = embed_model.encode(x_test)\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base')\n",
    "     \n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch, padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
    "\n",
    "X_train_xlm = tokenize(x_train)  # returns dictionary with two key:value. input ids:tensors and attention mask:tensors both of them contain tensors\n",
    "X_test_xlm = tokenize(x_test)\n",
    "\n",
    "a = X_train_xlm['input_ids'].size()\n",
    "b = X_test_xlm['input_ids'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "15ff6126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document-term matrix\n",
      "\n",
      "count based vectors - cv & tfidf:\n",
      " ((4015, 34266), (1004, 34266)) \n",
      " ((4015, 34266), (1004, 34266))\n",
      "\n",
      "embedding - sentence transformer:\n",
      " ((4015, 512), (1004, 512))\n",
      "\n",
      "embedding - xlm roberta:\n",
      " torch.Size([4015, 512]) torch.Size([1004, 512])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('document-term matrix\\n')\n",
    "print(f'count based vectors - cv & tfidf:\\n {X_train_cv.shape, X_test_cv.shape} \\n {X_train_tfidf.shape, X_test_tfidf.shape}\\n')\n",
    "print(f'embedding - sentence transformer:\\n {embedding_train.shape, embedding_test.shape}\\n')\n",
    "print(f'embedding - xlm roberta:\\n {a} {b}\\n') # size is batch_size, n_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "41e93c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "xlm vocabulary size: 250002 \n",
      "model context size: 512\n",
      "model input ['input_ids', 'attention_mask']\n",
      "\n",
      "xlm input ids:\n",
      " tensor([[    0, 99536, 43269,  ...,     1,     1,     1],\n",
      "        [    0, 48643,  4503,  ...,     1,     1,     1],\n",
      "        [    0,     6,  7456,  ...,     1,     1,     1],\n",
      "        ...,\n",
      "        [    0,  7828,   318,  ...,     1,     1,     1],\n",
      "        [    0, 68541,   655,  ...,     1,     1,     1],\n",
      "        [    0,  5456, 16386,  ...,     1,     1,     1]])\n",
      "xlm attention masks:\n",
      " tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])\n",
      "\n",
      "let's explore an example:\n",
      "\n",
      "torch.Size([512])\n",
      "tensor([     0,  99536,  43269,  43185,   1803,   9247,    219,  34573,   2638,\n",
      "         62175,   6973,  73140,  43185,   1803,   9247,    219,  34573,   2638,\n",
      "         62175,    520,  46391,  15595, 146684,  16757,    246, 161764,     28,\n",
      "         36716,  11951,   4681,  15275,  96233, 104843,  98174,     85,   5795,\n",
      "         17245,     42,     24,   4501,  39864,   3911, 110787,  41767,   3812,\n",
      "         45849,  72379,    437,    107,    206,   4823, 163717, 128673,   2265,\n",
      "         64498,  50061,  60616,  28046,    867,  63571,  26216,  17393,  83634,\n",
      "        129936,  21873,  52028,  41056,  29830,  33757,  31272,  21659, 188883,\n",
      "         49539,  43185,   1803,  77734, 188190, 189072,  16386,  59687,  68367,\n",
      "         82035,  64498,  21959,  39864,   1150,  96689, 129936,  45280,  64498,\n",
      "          2147,  12828,    102,  89652,     53,  21873, 186526, 165863, 153450,\n",
      "          3812,  69275, 206431,   1150,  46391,  15595, 146684,  11544,  18691,\n",
      "         97228,    846,  45849,     31,  43269,  73050,  43185,   1803,     91,\n",
      "         83018,  22917, 107580,  96233,    117,  22154,  79322, 123191,   8650,\n",
      "         96233,    520,  86466,  84360,   4681, 152236,  16386,  77734, 188190,\n",
      "           520,     24,   5600,  39864,   3911,  13461, 119218,     42,  66884,\n",
      "        147172,  81422,  89319, 186526,   1380,   1084,   9030,    520,  13120,\n",
      "           991,      2,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1])\n",
      "['<s>', '▁flora', '▁útil', '▁cafe', 'tal', '▁sier', 'ra', '▁norte', '▁pu', 'ebla', '▁mé', 'xico', '▁cafe', 'tal', '▁sier', 'ra', '▁norte', '▁pu', 'ebla', '▁ser', '▁agro', 'eco', 'sistema', '▁varia', 'do', '▁composición', '▁e', 'struct', 'uro', '▁poder', '▁encontrar', '▁especie', '▁vegetal', '▁cultiva', 'da', '▁sil', 'vest', 'r', '▁na', 'tiva', '▁introduc', 'ido', '▁cuyo', '▁divers', 'idad', '▁flor', 'ístico', '▁est', 're', 'ch', 'amente', '▁ligado', '▁condición', '▁social', '▁económico', '▁ec', 'ológico', '▁inser', 'tar', '▁producción', '▁café', '▁último', '▁década', '▁cultivo', '▁planta', '▁crisis', '▁debido', '▁bajo', '▁precio', '▁producto', '▁embargo', '▁agricultor', '▁mantener', '▁cafe', 'tal', '▁fuente', '▁ingreso', '▁adoptar', '▁nuevo', '▁estrategia', '▁obtener', '▁recurso', '▁económico', '▁mediante', '▁introduc', 'ción', '▁incremento', '▁cultivo', '▁importancia', '▁económico', '▁pi', 'mien', 'ta', '▁mame', 'y', '▁planta', '▁medicinal', '▁aprovechar', '▁versatil', 'idad', '▁posibilidad', '▁reorganiza', 'ción', '▁agro', 'eco', 'sistema', '▁trabajo', '▁presentar', '▁inventar', 'io', '▁flor', 'o', '▁útil', '▁encontrado', '▁cafe', 'tal', '▁s', 'np', '▁fecha', '▁registrado', '▁especie', '▁per', 'tene', 'ciente', '▁género', '▁familia', '▁especie', '▁ser', '▁objeto', '▁comercio', '▁poder', '▁representar', '▁nuevo', '▁fuente', '▁ingreso', '▁ser', '▁na', 'tivo', '▁introduc', 'ido', '▁haber', '▁agrupa', 'r', '▁categoría', '▁antropo', 'cén', 'trica', '▁medicinal', '▁come', 'sti', 'bl', '▁ser', '▁numero', 'so', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "# transformers \n",
    "\n",
    "print(torch.is_tensor(X_train_xlm))\n",
    "type(X_train_xlm)\n",
    "\n",
    "print(f'xlm vocabulary size: {tokenizer.vocab_size} \\nmodel context size: {tokenizer.model_max_length}\\nmodel input {tokenizer.model_input_names}\\n')\n",
    "print(f'xlm input ids:\\n {X_train_xlm.input_ids}\\nxlm attention masks:\\n {X_train_xlm.attention_mask}\\n')\n",
    "\n",
    "print('let\\'s explore an example:\\n')\n",
    "print(X_train_xlm['input_ids'][10].size())\n",
    "\n",
    "# the input sequence of each batch is padded [1] to the maximum sequence length in the batch (model context size)\n",
    "# the attention mask array is used to ignore the paddded areas of the betch \n",
    "print(X_train_xlm['input_ids'][0])\n",
    "print(tokenizer.convert_ids_to_tokens(X_train_xlm['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c8893e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden states from XLM are trained along with the parametres from the input data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c413261",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eed5271",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878cb394",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6347d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "xlm_clf = pipeline('text classifier', model = model_xlm, tokenizer=tokenizer)\n",
    "outputs = xlm_clf(**X_train_xlm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3865b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attributes of a tensor\n",
    "\n",
    "tensor = torch.rand(3,4)\n",
    "\n",
    "print(f\"Shape of tensor: {tensor.shape}\")\n",
    "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
    "print(f\"Device tensor is stored on: {tensor.device}\")\n",
    "\n",
    "# standard numpy-like indexing and slicing:\n",
    "\n",
    "tensor = torch.ones(4, 3)\n",
    "print(tensor)\n",
    "print(f\"First row: {tensor[0]}\")\n",
    "print(f\"First column: {tensor[:, 0]}\")\n",
    "print(f\"Last column: {tensor[..., -1]}\")\n",
    "tensor[:,1] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f358a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"hola soy Violeta!\"\n",
    "token_id = tokenizer(text)\n",
    "print(token_id)\n",
    "token = tokenizer.convert_ids_to_tokens(token_id.input_ids)\n",
    "print(token)\n",
    "print(tokenizer.convert_tokens_to_string(token))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222d3dac",
   "metadata": {},
   "source": [
    "# class imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a80aed",
   "metadata": {},
   "source": [
    "the distribution of one class is highly skewed so the learning algorithm might \n",
    "tend to be biased towards the majority class leading to poor predictions for the minority class\n",
    "\n",
    " approaches to deal with imbalanced datasets are:\n",
    " \n",
    "- undersample majority class: discards potentially valuable data\n",
    "- oversample minority class: can lead to overfitting and increases training times\n",
    "- weight loss function: assigns higher importance to minority classes providing a direct optimisation approach.\n",
    "sample: wj will be high\n",
    "majority sample: wj will be low\n",
    "- synthetic datasets: could be generated to complement the the minority class and increase its representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c03373",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# calculate weights for classes penalty (weighted cross-entropy loss)\n",
    "\n",
    "\"\"\"weight_for_class_i = total_samples / (num_samples_in_class_i * num_classes)\"\"\"\n",
    "\n",
    "weight_for_class_0 = len(x) / (len(neg) * 2) \n",
    "weight_for_class_1 = len(x) / (len(pos) * 2) \n",
    "\n",
    "print(weight_for_class_0, weight_for_class_1)\n",
    "\n",
    "# resample vectorised x_train and y_train - > returns list of arrays\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.over_sampling import ADASYN # generates synthetic samples in regions of the minority class where the class density is low\n",
    "\n",
    "rus = RandomUnderSampler(random_state=42, sampling_strategy=1)\n",
    "ros = RandomOverSampler(random_state=42, sampling_strategy='not majority')\n",
    "ada = ADASYN(random_state=42)\n",
    "\n",
    "resampler = [rus, ros, ada]\n",
    "\n",
    "# train set\n",
    "\n",
    "res_cv = []\n",
    "for x in resampler:\n",
    "    res_cv.append(x.fit_resample(X_train_cv, np.array(y_train)))\n",
    "    \n",
    "res_tfidf = []\n",
    "for x in resampler:\n",
    "    res_tfidf.append(x.fit_resample(X_train_tfidf, np.array(y_train)))\n",
    "\n",
    "# test set\n",
    "\n",
    "res_test_cv = []\n",
    "for x in resampler:\n",
    "    res_test_cv.append(x.fit_resample(X_test_cv, np.array(y_test)))\n",
    "    \n",
    "res_test_tfidf = []\n",
    "for x in resampler:\n",
    "    res_test_tfidf.append(x.fit_resample(X_test_tfidf, np.array(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11976724",
   "metadata": {},
   "source": [
    "# model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8425e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "logreg = LogisticRegression(solver='lbfgs')\n",
    "svm = SVC(kernel='linear')\n",
    "logreg_weight = LogisticRegression(solver='liblinear', class_weight={0: weight_for_class_0, 1: weight_for_class_1})\n",
    "svm_weight = SVC(kernel='linear', class_weight={0: weight_for_class_0, 1: weight_for_class_1})\n",
    "\n",
    "Case = namedtuple(\"Case\", \"name model X Y x y vector weighting\")\n",
    "\n",
    "cases_baseline = [\n",
    "    Case(name='Logistic Regression', model=logreg, X=X_train_cv, Y=y_train, x=X_test_cv, y=y_test, vector=\"word_frequency\", weighting='None'),\n",
    "    Case(name='SVM', model=svm, X=X_train_cv, Y=y_train, x=X_test_cv, y=y_test, vector=\"word_frequency\", weighting='None'),\n",
    "    Case(name='Logistic Regression', model=logreg, X=X_train_tfidf, Y=y_train, x=X_test_tfidf, y=y_test, vector=\"TF-IDF\", weighting='None'),\n",
    "    Case(name='SVM', model=svm, X=X_train_tfidf, Y=y_train, x=X_test_tfidf, y=y_test, vector=\"TF-IDF\", weighting='None'),\n",
    "    Case(name='Logistic Regression', model=logreg, X=embedding_train, Y=y_train, x=embedding_test, y=y_test, vector=\"embedding\", weighting='None'),\n",
    "    Case(name='SVM', model=svm, X=embedding_train, Y=y_train, x=embedding_test, y=y_test, vector=\"embedding\", weighting='None')\n",
    "]\n",
    "\n",
    "cases_weighted = [\n",
    "    Case(name='Logistic Regression', model=logreg_weight, X=X_train_cv, Y=y_train, x=X_test_cv, y=y_test, vector=\"word_frequency\", weighting='BAL'),\n",
    "    Case(name=\"SVM\", model=svm_weight, X=X_train_cv, Y=y_train, x=X_test_cv, y=y_test, vector=\"word_frequency\", weighting='BAL'),\n",
    "    Case(name='Logistic Regression', model=logreg_weight, X=X_train_tfidf, Y=y_train, x=X_test_tfidf, y=y_test, vector=\"TF-IDF\", weighting='BAL'),\n",
    "    Case(name=\"SVM\", model=svm_weight, X=X_train_tfidf, Y=y_train, x=X_test_tfidf, y=y_test, vector=\"TF-IDF\", weighting='BAL'),\n",
    "    Case(name='Logistic Regression', model=logreg_weight, X=embedding_train, Y=y_train, x=embedding_test, y=y_test, vector=\"embedding\", weighting=\"BAL\"),\n",
    "    Case(name='SVM', model=svm_weight, X=embedding_train, Y=y_train, x=embedding_test, y=y_test, vector=\"embedding\", weighting=\"BAL\")\n",
    "]\n",
    "\n",
    "cases_resampled_tfidf = [\n",
    "    Case(name=\"Logistic Regression\", model=logreg, X=res_tfidf[0][0], Y=res_tfidf[0][1], x=res_test_tfidf[0][0], y=res_test_tfidf[0][1], vector=\"TF-IDF\", weighting='RUS'),\n",
    "    Case(name=\"SVM\", model=svm, X=res_tfidf[0][0], Y=res_tfidf[0][1], x=res_test_tfidf[0][0], y=res_test_tfidf[0][1], vector=\"TF-IDF\", weighting='RUS'),\n",
    "    Case(name=\"Logistic Regression\", model=logreg, X=res_tfidf[1][0], Y=res_tfidf[1][1], x=res_test_tfidf[1][0], y=res_test_tfidf[1][1], vector=\"TF-IDF\", weighting='ROS'),\n",
    "    Case(name=\"SVM\", model=svm, X=res_tfidf[1][0], Y=res_tfidf[1][1], x=res_test_tfidf[1][0], y=res_test_tfidf[1][1], vector=\"TF-IDF\", weighting='ROS'),\n",
    "    Case(name=\"Logistic Regression\", model=logreg, X=res_tfidf[2][0], Y=res_tfidf[2][1], x=res_test_tfidf[2][0], y=res_test_tfidf[2][1], vector=\"TF-IDF\", weighting='ADA'),\n",
    "    Case(name=\"SVM\", model=svm, X=res_tfidf[2][0], Y=res_tfidf[2][1], x=res_test_tfidf[2][0], y=res_test_tfidf[2][1], vector=\"TF-IDF\", weighting='ADA')\n",
    "]\n",
    "\n",
    "cases_resampled_cv = [\n",
    "    Case(name=\"Logistic Regression\", model=logreg, X=res_cv[0][0], Y=res_cv[0][1], x=res_test_cv[0][0], y=res_test_cv[0][1],  vector=\"word_frequency\", weighting='RUS'),\n",
    "    Case(name=\"SVM\", model=svm, X=res_cv[0][0], Y=res_cv[0][1], x=res_test_cv[0][0], y=res_test_cv[0][1], vector=\"word_frequency\", weighting='RUS'),\n",
    "    Case(name=\"Logistic Regression\", model=logreg, X=res_cv[1][0], Y=res_cv[1][1], x=res_test_cv[1][0], y=res_test_cv[1][1], vector=\"word_frequency\", weighting='ROS'),\n",
    "    Case(name=\"SVM\", model=svm, X=res_cv[1][0], Y=res_cv[1][1], x=res_test_cv[1][0], y=res_test_cv[1][1], vector=\"word_frequency\", weighting='ROS'),\n",
    "    Case(name=\"Logistic Regression\", model=logreg, X=res_cv[2][0], Y=res_cv[2][1], x=res_test_cv[2][0], y=res_test_cv[2][1], vector=\"word_frequency\", weighting='ADA'),\n",
    "    Case(name=\"SVM\", model=svm, X=res_cv[2][0], Y=res_cv[2][1], x=res_test_cv[2][0], y=res_test_cv[2][1], vector=\"word_frequency\", weighting='ADA')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4a71f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to fit models, make predictions with train and test sets, and store results in df\n",
    "\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score\n",
    "\n",
    "def get_scores(cases):\n",
    "    # create lists to store information of each model and their prediction scores for train and test set\n",
    "    scores_list = []\n",
    "    models_list = []\n",
    "\n",
    "    for case in cases:\n",
    "        \n",
    "        # fit models and make predictions\n",
    "        model = case.model.fit(case.X, case.Y)\n",
    "        y_train_pred = cross_val_predict(case.model, case.X, case.Y, cv=StratifiedKFold(10), random_state=42, method='predict')\n",
    "        y_test_pred = model.predict(case.x)\n",
    "\n",
    "        # save fitted model\n",
    "        models = {\n",
    "            \"name\": f'{case.name}_{case.weighting}_{case.vector}',\n",
    "            \"model\": model,\n",
    "            \n",
    "        }\n",
    "        \n",
    "        # save scores from predicitons\n",
    "        scores = {\n",
    "            'Classifier': case.name,\n",
    "            'Numeric_rep': case.vector,\n",
    "            'Weighting': case.weighting,\n",
    "            'Instances': len(case.Y),\n",
    "            'F1_tr': round(f1_score(case.Y, y_train_pred), 3),\n",
    "            'F1_ts': round(f1_score(case.y, y_test_pred), 3),\n",
    "            'Precision_tr': round(precision_score(case.Y, y_train_pred), 3),\n",
    "            'Precision_ts': round(precision_score(case.y, y_test_pred), 3),\n",
    "            'Recall_tr': round(recall_score(case.Y, y_train_pred), 3),\n",
    "            'Recall_ts': round(recall_score(case.y, y_test_pred), 3)\n",
    "        }\n",
    "        \n",
    "        # store each model's dict and prediciton scores' dict in a list\n",
    "        models_list.append(models)\n",
    "        scores_list.append(scores)\n",
    "        \n",
    "    return models_list, scores_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1decaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# fit models and make predictions\n",
    "\n",
    "models_baseline, scores_baseline = get_scores(cases_baseline)\n",
    "models_weighted, scores_weighted = get_scores(cases_weighted)\n",
    "models_resampled_tfidf, scores_resampled_tfidf = get_scores(cases_resampled_tfidf)\n",
    "models_resampled_cv, scores_resampled_cv = get_scores(cases_resampled_cv)\n",
    "\n",
    "df1 = pd.DataFrame(scores_baseline)\n",
    "df2 = pd.DataFrame(scores_weighted)\n",
    "df3 = pd.DataFrame(scores_resampled_tfidf)\n",
    "df4 = pd.DataFrame(scores_resampled_cv)\n",
    "\n",
    "results =  pd.concat([df1,df2,df3,df4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fc27fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run xlm-roberta model\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as nn\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer)\n",
    "\n",
    "model_xlm_seq = AutoModelForSequenceClassification.from_pretrained('xlm-roberta-base')\n",
    "model_xlm_tok = AutoModelForTokenClassification.from_pretrained('xlm-roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c578c8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores_xlm(model, batch):\n",
    "    with torch.no_grad(): # disables gradient calculation\n",
    "    outputs = model(**batch) ## ** unpacks the values in dictionary\n",
    "    print(outputs) # raw outputs, logits \n",
    "    predictions = nn.softmax(outputs.logits, dim=1) # check f or nn before softmax\n",
    "    print(f'predictions {predictions}')\n",
    "    labels = torch.argmax(predictions, dim=1)\n",
    "    print(f'labels{labels}') \n",
    "    labels = [model.config.id2label[label_id] for label_id in labels.tolist()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd3f8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e277e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# https://www.youtube.com/watch?v=GSt00_-0ncQ&t=192s\n",
    "\n",
    "model_xlm = AutoModelForSequenceClassification.from_pretrained('xlm-roberta-base')\n",
    "\n",
    "with torch.no_grad(): # disables gradient calculation\n",
    "    outputs = model_xlm(**X_test_xlm) ## ** unpacks the values in dictionary\n",
    "    print(outputs) # raw outputs, logits \n",
    "    predictions = nn.softmax(outputs.logits, dim=1) # check f or nn before softmax\n",
    "    print(f'predictions {predictions}')\n",
    "    labels = torch.argmax(predictions, dim=1)\n",
    "    print(f'labels{labels}') \n",
    "    labels = [model.config.id2label[label_id] for label_id in labels.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2833184",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = results.reset_index(drop=True).sort_values('F1_ts', ascending=False)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c124c08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d792b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "results[results['Classifier'] == 'SVM'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb6f702",
   "metadata": {},
   "outputs": [],
   "source": [
    "results[results['Classifier'] == 'Logistic Regression']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2239738a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract models form liest of dicts\n",
    "\n",
    "print(models_weighted)\n",
    "model_list = [x[\"model\"] for x in models_weighted]\n",
    "print(model_list)\n",
    "\n",
    "d = {model[\"name\"]:model for model in models_weighted}\n",
    "\n",
    "d[\"Logistic Regression-word_frequency-BAL\"][\"input\"] = \n",
    "\n",
    "# best_model = next(model for model in models_weighted if model[\"name\"] == 'Logistic Regression-embedding-BAL')\n",
    "# best_model = best_model['model']\n",
    "# best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57b560d",
   "metadata": {},
   "source": [
    "# xlm-roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f8168d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run end to end model usinf transformer architechture - sentence transformers and \n",
    "# XLMRobertaForSequenceClassification: XLM-RoBERTa Model transformer with a sequence classification/regression head on top (a linear layer on top of the pooled output) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13d5264",
   "metadata": {},
   "source": [
    "# selected model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2788c9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_train = embed_model.encode(x_train)\n",
    "embedding_test = embed_model.encode(x_test)\n",
    "\n",
    "mod = LogisticRegression(solver='lbfgs', class_weight={0: weight_for_class_0, 1: weight_for_class_1}).fit(embedding_train,y_train)\n",
    "y_train_predict = cross_val_predict(mod, embedding_train, y_train, cv=StratifiedKFold(10), method='predict')\n",
    "y_test_pred = mod.predict(embedding_test)\n",
    "\n",
    "y_test_pred = mod.predict(embedding_test)\n",
    "print(f'Accuracy of logistic regression classifier on test set: {mod.score(embedding_test, y_test)}')\n",
    "        \n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a679f109",
   "metadata": {},
   "source": [
    "# confusion matrix\n",
    "\n",
    "- true negatives, false positives\n",
    "- false negatives, true positives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02eb2162",
   "metadata": {},
   "source": [
    "# modify threshold\n",
    "\n",
    "https://stackoverflow.com/questions/35692059/how-to-change-threshold-for-precision-and-recall-in-python-scikit-learn\n",
    "\n",
    "\n",
    "https://stackoverflow.com/questions/31417487/sklearn-logisticregression-and-changing-the-default-threshold-for-classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee05747",
   "metadata": {},
   "source": [
    "`decision_function`: tells us on which side of the hyperplane generated by the classifier samples in X are (and how far we are away from it)\n",
    "\n",
    "\n",
    "`predict()` : returns class label for samples in X based on the values generated by the decision function\n",
    "\n",
    "\n",
    "`predic_proba()` : returns probability estimates of a classification label for samples in X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf273365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # to do\n",
    "# try different thresholds ~ 50\n",
    "# make predictions\n",
    "# calculate precision, recall, f1 for these and compare\n",
    "# plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c043cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decision funtion\n",
    "print(model.decision_function(X_train_cv[:200]))\n",
    "\n",
    "# probability classes \n",
    "print(model.predict(X_train_cv[:200]))\n",
    "\n",
    "# probability estimates\n",
    "probabilities = model.predict_proba(X_train_cv)\n",
    "print(probabilities[:200])\n",
    "probabilities.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4c757d",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.01\n",
    "preds = (probabilities[:, 1] > threshold)\n",
    "preds = preds.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0919bb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify threshold for probabiity estimates\n",
    "high_precision = (probabilities > 0.7).astype(int)\n",
    "high_recall = (probabilities > 0.2).astype(int)\n",
    "\n",
    "print(high_precision[:200], high_recall[:200])\n",
    "# # # true = high_recall == 0\n",
    "# # len(high_recall)\n",
    "# type(probabilities)\n",
    "# probabilities.shape\n",
    "# probabilities[0][0]\n",
    "\n",
    "# decisions = (model.predict_proba() >= mythreshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0009989e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate precision, recall, and F1\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "precision = precision_score(y_train, y_train_pred)\n",
    "recall = recall_score(y_train, y_train_pred)\n",
    "f1 = recall_score(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca1ed5a",
   "metadata": {},
   "source": [
    "# checking feature weights of positive and negative classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44a4194",
   "metadata": {},
   "source": [
    "what words have high positive and negative weights in possitive class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e197cfdb",
   "metadata": {},
   "source": [
    "check if keywords appear in doc\n",
    "https://machinelearningmastery.com/calculate-feature-importance-with-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6e6435",
   "metadata": {},
   "outputs": [],
   "source": [
    "Linear machine learning algorithms fit a model where the prediction is the weighted sum of the input values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e4292f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58085a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get feature importance\n",
    "\n",
    "coeffs = model.coef_[0]\n",
    "\n",
    "print(len(coeffs))\n",
    "\n",
    "# summarize feature importance\n",
    "\n",
    "for i,v in enumerate(coeffs):\n",
    " print('Feature: %0d, Score: %.5f' % (i,v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488392b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit_transform(x_train)\n",
    "\n",
    "# extract feature names and coefficients\n",
    "\n",
    "features = vectorizer.get_feature_names_out(x_train)\n",
    "coeffs = model.coef_[0]\n",
    "print(features[5:10])  \n",
    "print(len(features))\n",
    "print(len(coeffs))\n",
    "\n",
    "feat_coeff_dict = {}\n",
    "feat_coeff_dict['_features'] = features\n",
    "feat_coeff_dict['_coefficients'] = coeffs\n",
    "\n",
    "feature_importance_df = pd.DataFrame.from_dict(feat_coeff_dict, orient = 'index').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf242bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "feature_importance_df.sort_values(by='_coefficients', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3388d1",
   "metadata": {},
   "source": [
    "# precison-recall trade-off"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fe5011",
   "metadata": {},
   "source": [
    "plotting curve of precision / recall trade-off\n",
    "\n",
    "the higher the precison, the lower the recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b429fc29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe42862",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_train, y_scores, pos_label='1')\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n",
    "    plt.plot(thresholds,precisions[:-1],\"b-\",label='Precision')\n",
    "    plt.plot(thresholds,recalls[:-1],\"g-\",label='Recall')\n",
    "\n",
    "plot_precision_recall_vs_threshold(precisions, recalls, thresholds)\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.legend(['Precision', 'Recall'], loc =\"right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc274855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot precision-recall curve \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "ax.plot(recalls, precisions, label='Logistic Regression')\n",
    "baseline = len(y_test[y_test==1])/len(y_test)\n",
    "ax.plot([0, 1], [baseline, baseline], linestyle='--', label='Baseline')\n",
    "ax.set_xlabel('Recall')\n",
    "ax.set_ylabel('Precision')\n",
    "ax.legend(loc='center left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a2e767",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import auc\n",
    "\n",
    "auc(recalls, precisions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2644c6",
   "metadata": {},
   "source": [
    "# ROC curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4d4e32",
   "metadata": {},
   "source": [
    "- Plots sensitivity vs specificity\n",
    "- This is True Positive Rate (recall - TP/TP+FN) vs True Negative Rate (TN/TN+FP)\n",
    "- ROC is a probability curve\n",
    "- AUC represents the degree or measure of separability. \n",
    "- Higher the AUC, the better the model is at predicting 0 classes as 0 and 1 classes as 1. \n",
    "- It tells how much the model is capable of distinguishing between classes as its discrimination threshold is varied. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43161ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "roc_score = roc_auc_score(y_train, y_scores)\n",
    "\n",
    "roc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a921dc6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_train, y_scores, pos_label='1 ')\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_roc_curve (fpr, tpr, label=None):\n",
    "    plt.plot(fpr, tpr, linewidth=2, label=label)\n",
    "    plt.plot([0,1],[0,1],'k--') # dashed diagonal\n",
    "    \n",
    "plot_roc_curve(fpr,tpr)\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "ax.legend(loc='center left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d9204c",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'vocabulary size': len(vocab), 'mean_cross_val': np.mean(scores), 'precision': precision, \"recall\": recall, \"f1_score\": f1_score, \"roc_score\": roc_score}\n",
    "df = pd.DataFrame(data=d, index=[\"body\"])\n",
    "# df.to_csv('/Users/uqvberde/Dropbox/TRANSLATE/Objective 2 - Machine Learning/classifier_spanish/datasets/py_outputs/metrics_9.csv')\n",
    "df.to_csv('C:\\\\Users\\\\uqvberde\\\\Dropbox\\\\TRANSLATE\\\\ML\\\\classifier_spanish\\\\datasets\\\\py_outputs\\\\metrics\\\\metrics_69_short.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cc0a2e",
   "metadata": {},
   "source": [
    "`C regularisation`: hyperparametre. Regularization generally refers the concept that there should be a complexity penalty for more extreme parameters. The idea is that just looking at the training data and not paying attention to how extreme one's parameters are leads to overfitting. A high value of C tells the model to give high weight to the training data, and a lower weight to the complexity penalty. A low value tells the model to give more weight to this complexity penalty at the expense of fitting to the training data. Basically, a high C means \"Trust this training data a lot\", while a low value says \"This data may not be fully representative of the real world data, so if it's telling you to make a parameter really large, don't listen to it\".\n",
    "\n",
    "https://stackoverflow.com/questions/67513075/what-is-c-parameter-in-sklearn-logistic-regression\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
